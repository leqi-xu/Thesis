{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Knowledge Graph dataset pre-processing functions.\"\"\"\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "os.environ[\"DATA_PATH\"] = \"/mnt/c/Users/96551/Documents/Master Arbeit/KGEmb-master/data\"\n",
    "\n",
    "def get_idx(path):\n",
    "    \"\"\"Map entities and relations to unique ids.\n",
    "\n",
    "    Args:\n",
    "      path: path to directory with raw dataset files (tab-separated train/valid/test triples)\n",
    "\n",
    "    Returns:\n",
    "      ent2idx: Dictionary mapping raw entities to unique ids\n",
    "      rel2idx: Dictionary mapping raw relations to unique ids\n",
    "      ent_counts: Dictionary mapping entities to their counts\n",
    "      rel_counts: Dictionary mapping relations to their counts\n",
    "    \"\"\"\n",
    "    # 提取出所有的实体和关系，为每个实体和关系分配一个唯一的整数标识符，构建 ent2idx 和 rel2idx 字典\n",
    "    entities, relations = set(), set()\n",
    "    entity_counts = defaultdict(int)\n",
    "    relation_counts = defaultdict(int)\n",
    "    for split in [\"train30\", \"valid\", \"test70\"]:\n",
    "        with open(os.path.join(path, split), \"r\") as lines:\n",
    "            for line in lines:\n",
    "                lhs, rel, rhs = line.strip().split(\"\\t\")\n",
    "                entities.add(lhs)\n",
    "                entities.add(rhs)\n",
    "                relations.add(rel)\n",
    "\n",
    "                if split == \"train30\":\n",
    "                    entity_counts[lhs] += 1\n",
    "                    entity_counts[rhs] += 1\n",
    "                    relation_counts[rel] += 1\n",
    "                  \n",
    "    ent2idx = {x: i for (i, x) in enumerate(sorted(entities))}\n",
    "    rel2idx = {x: i for (i, x) in enumerate(sorted(relations))}\n",
    "\n",
    "    for split in [\"train30\"]:\n",
    "        if split == \"train30\":\n",
    "            ent2idx_counts = {ent2idx[ent]: count for ent, count in entity_counts.items()}\n",
    "            rel2idx_counts = {rel2idx[rel]: count for rel, count in relation_counts.items()}\n",
    "\n",
    "            with open(os.path.join(path, split + '_ent2idx_counts.json'), 'w') as f:\n",
    "                json.dump(ent2idx_counts, f)\n",
    "\n",
    "            with open(os.path.join(path, split + '_rel2idx_counts.json'), 'w') as f:\n",
    "                json.dump(rel2idx_counts, f)             \n",
    "\n",
    "    return ent2idx, rel2idx\n",
    "\n",
    "# 将原始的三元组映射到一个Numpy数组，其中实体和关系被转换为唯一的整数标识\n",
    "def to_np_array(dataset_file, ent2idx, rel2idx):\n",
    "    \"\"\"Map raw dataset file to numpy array with unique ids.\n",
    "\n",
    "    Args:\n",
    "      dataset_file: Path to file containing raw triples in a split\n",
    "      ent2idx: Dictionary mapping raw entities to unique ids\n",
    "      rel2idx: Dictionary mapping raw relations to unique ids\n",
    "\n",
    "    Returns:\n",
    "      Numpy array of size n_examples x 3 mapping the raw dataset file to ids\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    with open(dataset_file, \"r\") as lines:\n",
    "        for line in lines:\n",
    "            lhs, rel, rhs = line.strip().split(\"\\t\")\n",
    "            try:\n",
    "                examples.append([ent2idx[lhs], rel2idx[rel], ent2idx[rhs]])\n",
    "                \n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    return np.array(examples).astype(\"int64\")\n",
    "\n",
    "# 计算三元组的sampling score, 根据得分进行排序\n",
    "def examples_sampling_score(triples, ent2idx_counts, rel2idx_counts):\n",
    "    ranked_examples = []\n",
    "    for triple in triples:\n",
    "        lhs, rel, rhs = map(str, triple)\n",
    "        sampling_score = ent2idx_counts[lhs] * rel2idx_counts[rel] * ent2idx_counts[rhs]\n",
    "        ranked_examples.append([int(lhs), int(rel), int(rhs), sampling_score])\n",
    "\n",
    "    # 处理对称关系\n",
    "    symmetrical_examples = []\n",
    "    for example in ranked_examples:\n",
    "        lhs, rel, rhs, score = example\n",
    "        symmetrical_score = score  # 保留原始关系的分数\n",
    "        symmetrical_rel = rel + len(rel2idx_counts)  # 对称关系的索引调整\n",
    "        symmetrical_examples.append([rhs, symmetrical_rel, lhs, symmetrical_score])\n",
    "    \n",
    "    ranked_examples.extend(symmetrical_examples)\n",
    "    ranked_examples = sorted(ranked_examples, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "    probabilities = []\n",
    "    alpha = -1\n",
    "    total_examples = len(ranked_examples)\n",
    "    total_rank_alpha_sum = sum([math.log(rank ** alpha + 1) for rank in range(1, total_examples + 1)])\n",
    "    for rank, triple in enumerate(ranked_examples, start=1):\n",
    "        rank_alpha = (math.log(rank ** alpha + 1)) ** alpha\n",
    "        probability = rank_alpha / total_rank_alpha_sum\n",
    "        probabilities.append(probability)\n",
    "    probabilities_tensor = torch.tensor(probabilities)\n",
    "\n",
    "    ranked_examples_score = [example[-1] for example in ranked_examples]\n",
    "    ranked_examples = [example[:-1] for example in ranked_examples]\n",
    "    ranked_examples = torch.tensor(ranked_examples)\n",
    "\n",
    "    return ranked_examples, ranked_examples_score, probabilities_tensor\n",
    "        \n",
    "\n",
    "# 创建过滤器，以确保预测的实体不在训练集中\n",
    "def get_filters(examples, n_relations):\n",
    "    \"\"\"Create filtering lists for evaluation.\n",
    "\n",
    "    Args:\n",
    "      examples: Numpy array of size n_examples x 3 containing KG triples\n",
    "      n_relations: Int indicating the total number of relations in the KG\n",
    "\n",
    "    Returns:\n",
    "      lhs_final: Dictionary mapping queries (entity, relation) to filtered entities for left-hand-side prediction\n",
    "      rhs_final: Dictionary mapping queries (entity, relation) to filtered entities for right-hand-side prediction\n",
    "    \"\"\"\n",
    "    lhs_filters = collections.defaultdict(set)\n",
    "    rhs_filters = collections.defaultdict(set)\n",
    "    for lhs, rel, rhs in examples:\n",
    "        rhs_filters[(lhs, rel)].add(rhs)\n",
    "        # 确保左侧过滤器的键与右侧过滤器的键不同\n",
    "        lhs_filters[(rhs, rel + n_relations)].add(lhs)\n",
    "    lhs_final = {}\n",
    "    rhs_final = {}\n",
    "    for k, v in lhs_filters.items():\n",
    "        lhs_final[k] = sorted(list(v))\n",
    "    for k, v in rhs_filters.items():\n",
    "        rhs_final[k] = sorted(list(v))\n",
    "    return lhs_final, rhs_final\n",
    "\n",
    "\n",
    "def process_dataset(path):\n",
    "    \"\"\"Map entities and relations to ids and saves corresponding pickle arrays.\n",
    "\n",
    "    Args:\n",
    "      path: Path to dataset directory\n",
    "\n",
    "    Returns:\n",
    "      examples: Dictionary mapping splits to with Numpy array containing corresponding KG triples.\n",
    "      filters: Dictionary containing filters for lhs and rhs predictions.\n",
    "    \"\"\"\n",
    "    ent2idx, rel2idx = get_idx(dataset_path)\n",
    "    examples = {}\n",
    "    splits = [\"train30\", \"valid\", \"test70\"]\n",
    "    for split in splits:\n",
    "        dataset_file = os.path.join(path, split)\n",
    "        examples[split] = to_np_array(dataset_file, ent2idx, rel2idx)\n",
    "\n",
    "        if split == \"train30\":\n",
    "            train_examples = examples[split]\n",
    "\n",
    "            with open(os.path.join(path, split + '_ent2idx_counts.json'), 'r') as f:\n",
    "                ent2idx_counts = json.load(f)\n",
    "            with open(os.path.join(path, split + '_rel2idx_counts.json'), 'r') as f:\n",
    "                rel2idx_counts = json.load(f)\n",
    "\n",
    "            ranked_examples, ranked_examples_score, probabilities_tensor = examples_sampling_score(train_examples, ent2idx_counts, rel2idx_counts)\n",
    "\n",
    "    all_examples = np.concatenate([examples[split] for split in splits], axis=0)\n",
    "    lhs_skip, rhs_skip = get_filters(all_examples, len(rel2idx))\n",
    "    filters = {\"lhs\": lhs_skip, \"rhs\": rhs_skip}\n",
    "\n",
    "    return examples, filters, ranked_examples, ranked_examples_score, probabilities_tensor\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = os.environ[\"DATA_PATH\"]\n",
    "    for dataset_name in os.listdir(data_path):\n",
    "        if dataset_name == 'FB237':\n",
    "            dataset_path = os.path.join(data_path, dataset_name)\n",
    "            dataset_examples, dataset_filters, ranked_examples, ranked_examples_score, probabilities_tensor = process_dataset(dataset_path)\n",
    "            for dataset_split in [\"train30\", \"valid\", \"test70\"]:\n",
    "                save_path = os.path.join(dataset_path, dataset_split + \".pickle\")\n",
    "                with open(save_path, \"wb\") as save_file:\n",
    "                    pickle.dump(dataset_examples[dataset_split], save_file)\n",
    "            with open(os.path.join(dataset_path, \"to_skip.pickle\"), \"wb\") as save_file:\n",
    "                pickle.dump(dataset_filters, save_file)\n",
    "\n",
    "        # 保存 probabilities_tensor\n",
    "        torch.save(probabilities_tensor, os.path.join(dataset_path, \"probabilities_tensor_symmetrical.pt\"))\n",
    "            \n",
    "        # 保存 ranked_examples_array\n",
    "        with open(os.path.join(dataset_path, \"ranked_examples_tensor_symmetrical.pickle\"), \"wb\") as save_file:\n",
    "            pickle.dump(ranked_examples, save_file)\n",
    "        \n",
    "        # 保存 ranked_examples_score\n",
    "        with open(os.path.join(dataset_path, \"ranked_examples_score_symmetrical.pickle\"), \"wb\") as save_file:\n",
    "            pickle.dump(ranked_examples_score, save_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
